# ========================================
# F&O ETL Pipeline - Environment Configuration
# ========================================
# Copy this file to .env and fill in your values

# ----------------------------------------
# Zerodha Kite API Credentials
# ----------------------------------------
# Get these from https://kite.trade/
KITE_API_KEY=your_api_key_here
KITE_API_SECRET=your_api_secret_here
KITE_ACCESS_TOKEN=your_access_token_here
KITE_REQUEST_TOKEN=your_request_token_here

# ----------------------------------------
# Data Lake Configuration
# ----------------------------------------
# Local path (for Docker container)
DATA_LAKE_PATH=/usr/local/airflow/data/lake

# For Windows local development (outside Docker)
# DATA_LAKE_PATH=C:/Users/91629/Desktop/CodeForFun/stock-etl/data/lake

# ----------------------------------------
# F&O Instruments to Process
# ----------------------------------------
# Comma-separated list of underlyings
UNDERLYINGS=BANKNIFTY,NIFTY

# ----------------------------------------
# Trading Calendar
# ----------------------------------------
# Market timing (IST)
MARKET_OPEN_HOUR=9
MARKET_OPEN_MINUTE=15
MARKET_CLOSE_HOUR=15
MARKET_CLOSE_MINUTE=30

# ----------------------------------------
# DAG Configuration
# ----------------------------------------
# Number of parallel instruments to fetch
MAX_PARALLEL_INSTRUMENTS=5

# Retry configuration
DAG_RETRIES=2
DAG_RETRY_DELAY_MINUTES=5

# Backfill settings
DAG_CATCHUP=True
DAG_MAX_ACTIVE_RUNS=1

# ----------------------------------------
# Logging & Monitoring
# ----------------------------------------
LOG_LEVEL=INFO
ENABLE_METRICS=False

# ----------------------------------------
# Data Quality Thresholds
# ----------------------------------------
# Minimum expected candles per day per instrument (375 = 9:15 to 15:30)
MIN_CANDLES_PER_DAY=370

# Maximum allowed null percentage before failing
MAX_NULL_PERCENTAGE=1.0

# ----------------------------------------
# AWS S3 Configuration
# ----------------------------------------
# Required for S3 upload functionality
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
S3_BUCKET_NAME=your-bucket-name
S3_REGION=ap-south-1

# ----------------------------------------
# Nifty 500 Equity Pipeline Configuration
# ----------------------------------------
# Limit instruments for testing (0 = all 500)
NIFTY500_MAX_INSTRUMENTS=0

# ----------------------------------------
# Pipeline Orchestration
# ----------------------------------------
# Enable/disable individual pipelines
RUN_FNO_PIPELINE=true
RUN_EQUITY_PIPELINE=true

# Run pipelines in parallel (true) or sequential (false)
RUN_PIPELINES_PARALLEL=true

# ----------------------------------------
# Optional: GCS Configuration (Future)
# ----------------------------------------
# GCP_PROJECT_ID=
# GCS_BUCKET_NAME=

# ----------------------------------------
# Optional: Database Connection (Future)
# ----------------------------------------
# For analytics/reporting
# ANALYTICS_DB_URI=postgresql://user:pass@host:5432/db

# ----------------------------------------
# Airflow Variables (set via Airflow UI or CLI)
# ----------------------------------------
# AIRFLOW_VAR_NOTIFICATION_EMAIL=your@email.com
# AIRFLOW_VAR_ENABLE_SLACK_ALERTS=False
