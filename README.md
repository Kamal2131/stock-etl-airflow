# ğŸ“ˆ Stock Market ETL Pipeline

Production-grade ETL pipeline for ingesting market data using **Apache Airflow (Astronomer)** and storing in a **partitioned Parquet data lake** with AWS S3 upload.

## ğŸ¯ Features

| Pipeline | Data | Interval | Source |
|----------|------|----------|--------|
| **F&O ETL** | BANKNIFTY, NIFTY derivatives | 1-minute | Kite API |
| **Nifty 500 ETL** | 500 NSE equity stocks | 5-minute | Kite API |

- âœ… Daily automated runs at **4:00 PM IST**
- âœ… Partitioned **Parquet data lake** (idempotent, backfill-ready)
- âœ… **AWS S3** upload support
- âœ… **Master orchestrator** for coordinating pipelines

## ğŸ—ï¸ Architecture

```
Zerodha Kite API
        â†“
    [Extract]     â†’ Raw OHLCV from API
        â†“
   [Transform]    â†’ Clean, validate, enrich
        â†“
  [Load Raw]      â†’ data/lake/{fno|equity}/raw/
        â†“
   [Process]      â†’ data/lake/{fno|equity}/processed/
        â†“
  [Upload S3]     â†’ s3://bucket/nifty500/date=YYYY-MM-DD/
        â†“
[Quality Check]   â†’ Validate data integrity
```

## ğŸ“ Project Structure

```
stock-etl-airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ fno_etl_dag.py              # F&O derivatives ETL
â”‚   â”œâ”€â”€ nifty500_etl_dag.py         # Nifty 500 equity ETL
â”‚   â””â”€â”€ market_etl_orchestrator.py  # Master orchestrator
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ kite_extractor.py       # F&O Kite client
â”‚   â”‚   â””â”€â”€ nifty500_extractor.py   # Nifty 500 Kite client
â”‚   â”œâ”€â”€ transformers/
â”‚   â”‚   â”œâ”€â”€ fno_transformer.py      # F&O data cleaning
â”‚   â”‚   â””â”€â”€ equity_transformer.py   # Equity data cleaning
â”‚   â”œâ”€â”€ loaders/
â”‚   â”‚   â”œâ”€â”€ parquet_loader.py       # Local Parquet writer
â”‚   â”‚   â””â”€â”€ s3_loader.py            # AWS S3 uploader
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ data_quality.py         # Quality checks
â”œâ”€â”€ data/lake/
â”‚   â”œâ”€â”€ fno/                        # F&O data
â”‚   â””â”€â”€ equity/                     # Equity data
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites

- Docker Desktop
- Astro CLI (`winget install -e --id Astronomer.Astro`)
- Zerodha Kite API credentials

### Setup

```powershell
# Configure environment
copy .env.example .env
# Edit .env with your credentials

# Start Airflow
astro dev start

# Access UI: http://localhost:8080 (admin/admin)
```

## âš™ï¸ Configuration

Edit `.env` file:

```bash
# Kite API (required)
KITE_API_KEY=your_api_key
KITE_ACCESS_TOKEN=your_token

# AWS S3 (optional)
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret
S3_BUCKET_NAME=your-bucket

# Orchestration
RUN_FNO_PIPELINE=true
RUN_EQUITY_PIPELINE=true
RUN_PIPELINES_PARALLEL=true

# Testing (limit stocks)
NIFTY500_MAX_INSTRUMENTS=20
```

## â° DAG Schedule

| DAG | Schedule | Description |
|-----|----------|-------------|
| `market_etl_orchestrator` | 4:00 PM IST | Master - triggers both pipelines |
| `fno_etl_daily` | 4:00 PM IST | F&O derivatives only |
| `nifty500_etl_daily` | 4:00 PM IST | Nifty 500 equity only |

## ğŸ“Š Data Lake Structure

```
data/lake/
â”œâ”€â”€ fno/processed/underlying=BANKNIFTY/date=2025-01-06/data.parquet
â””â”€â”€ equity/processed/date=2025-01-06/data.parquet
```

Query with DuckDB:
```python
import duckdb
duckdb.sql("SELECT * FROM 'data/lake/equity/processed/**/*.parquet' LIMIT 10")
```

## ğŸ“ Commands

```powershell
astro dev start       # Start Airflow
astro dev stop        # Stop Airflow
astro dev restart     # Restart
astro dev run dags trigger nifty500_etl_daily  # Manual trigger
```

## ğŸ§ª Quality Checks

- Row count validation (75 candles/stock for 5-min)
- Unique `(symbol, timestamp)` constraint
- Market hours filtering (9:15 AM - 3:30 PM)
- OHLC relationship validation

---

**Tech Stack**: Python 3.10 | Apache Airflow 2.x | Astronomer | PyArrow | boto3 | DuckDB
