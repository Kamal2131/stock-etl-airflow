# ğŸ“ˆ Real-Time F&O Stock Market ETL Pipeline

Production-grade ETL pipeline for ingesting 1-minute F&O market data using **Apache Airflow (Astronomer)** and storing in a **partitioned Parquet data lake**.

## ğŸ¯ What This Does

- Downloads **1-minute OHLCV + Open Interest** data from Zerodha Kite API
- Runs **daily after market close** via Airflow scheduler
- Stores data in a **partitioned Parquet lake** (idempotent, backfill-ready)
- Supports **BANKNIFTY** and **NIFTY** F&O instruments

## ğŸ—ï¸ Architecture

```
Zerodha Kite API
        â†“
    [Extract]     â†’ Raw JSON from API
        â†“
   [Transform]    â†’ Clean, validate, enrich
        â†“
  [Load Raw]      â†’ data/lake/fno/raw/
        â†“
   [Process]      â†’ data/lake/fno/processed/
        â†“
[Quality Check]   â†’ Validate data integrity
```

## ğŸ“ Project Structure

```
stock-etl/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ fno_etl_dag.py          # Main ETL DAG
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â””â”€â”€ kite_extractor.py   # Kite API client
â”‚   â”œâ”€â”€ transformers/
â”‚   â”‚   â””â”€â”€ fno_transformer.py  # Data transformation
â”‚   â”œâ”€â”€ loaders/
â”‚   â”‚   â””â”€â”€ parquet_loader.py   # Parquet writer
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ data_quality.py     # Quality checks
â”œâ”€â”€ data/lake/fno/
â”‚   â”œâ”€â”€ raw/                    # Unprocessed data
â”‚   â”œâ”€â”€ processed/              # Cleaned data
â”‚   â””â”€â”€ analytics/              # Indicators (future)
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites

- Docker Desktop (running)
- Astro CLI (`winget install -e --id Astronomer.Astro`)
- Zerodha Kite API credentials

### Setup

```powershell
# 1. Configure environment
copy .env.example .env
# Edit .env with your Kite API credentials

# 2. Start Airflow
astro dev start

# 3. Access Airflow UI
# Open http://localhost:8080
# Login: admin / admin
```

### Trigger DAG

```powershell
# Manual trigger for today
astro dev run dags trigger fno_etl_daily

# Backfill historical date
astro dev run dags trigger fno_etl_daily --conf '{"ds": "2025-01-03"}'
```

## ğŸ“Š Data Lake Partitioning

```
data/lake/fno/processed/
â””â”€â”€ underlying=BANKNIFTY/
    â””â”€â”€ date=2025-01-06/
        â””â”€â”€ data.parquet
```

Query with DuckDB:
```python
import duckdb
duckdb.sql("SELECT * FROM 'data/lake/fno/processed/**/*.parquet' LIMIT 10")
```

## â° Schedule

| DAG | Schedule | Description |
|-----|----------|-------------|
| `fno_etl_daily` | `0 16 * * 1-5` | 4:00 PM IST, Mon-Fri |

## ğŸ§ª Quality Checks

- Row count validation
- Unique `(symbol, timestamp)` check
- Market hours filtering
- Price validity (no negative/inverted OHLC)

## ğŸ“ Commands Reference

```powershell
astro dev start     # Start Airflow
astro dev stop      # Stop Airflow
astro dev restart   # Restart Airflow
astro dev logs      # View logs
astro dev bash      # Shell into container
```

## ğŸ”® Future Extensions

- [ ] S3/GCS storage backend
- [ ] Analytics layer with indicators (VWAP, OI change)
- [ ] DuckDB dashboard
- [ ] Kafka streaming ingestion
- [ ] ML feature store

---

**Tech Stack**: Python 3.10 | Apache Airflow 2.x | Astronomer | PyArrow | DuckDB
